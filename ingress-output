kubectl get svc -n monitoring
NAME                                  TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
grafana                               ClusterIP   172.200.13.195    <none>        80/TCP     9d
prometheus-alertmanager               ClusterIP   172.200.731.127    <none>        9093/TCP   9d
prometheus-alertmanager-headless      ClusterIP   None             <none>        9093/TCP   9d
prometheus-kube-state-metrics         ClusterIP   172.200.184.217   <none>        8080/TCP   9d
prometheus-prometheus-node-exporter   ClusterIP   172.200.24.195    <none>        9100/TCP   9d
prometheus-prometheus-pushgateway     ClusterIP   172.200.35.64     <none>        9091/TCP   9d
prometheus-server                     ClusterIP   172.200.130.35    <none>        80/TCP     7d23h

kubectl get ing -n monitoring
NAME             CLASS   HOSTS   ADDRESS                                                                   PORTS   AGE
k8s-prometheus   nginx   *       ac179b04319b987294f71921bf2150a897f6-1409830007.us-east-1.elb.amazonaws.com   80      13m

kubectl describe ingress k8s-prometheus -n monitoring
Name:             k8s-prometheus
Labels:           app.kubernetes.io/managed-by=Helm
Namespace:        monitoring
Address:          ac179b04319b987294f71921bf2150a897f6-1409830007.us-east-1.elb.amazonaws.com
Ingress Class:    nginx
Default backend:  <default>
Rules:
  Host        Path  Backends
  ----        ----  --------
  *
              /prometheus(/|$)(.*)   prometheus-server:80 (10.244.41.209:9090)
              /grafana(/|$)(.*)      grafana:80 (10.244.42.202:3000)
              /alertmanager          prometheus-alertmanager:9093 (10.244.41.96:9093)
Annotations:  meta.helm.sh/release-name: monitoring
              meta.helm.sh/release-namespace: monitoring
              nginx.ingress.kubernetes.io/rewrite-target: /$2
Events:
  Type    Reason  Age                From                      Message
  ----    ------  ----               ----                      -------
  Normal  Sync    14m (x2 over 14m)  nginx-ingress-controller  Scheduled for sync
  Normal  Sync    14m (x2 over 14m)  nginx-ingress-controller  Scheduled for sync

---------------------------------------------
prometheus-values.yaml

prometheus:
  ingress:
    enabled: false
  prometheusSpec:
    externalUrl: "http://ac179b04319b94f71921bf2150a897f6-1409830007.us-east-1.elb.amazonaws.com/prometheus/"
    routePrefix: "/"
server:
  persistentVolume:
    enabled: true
    storageClass: "gp2"  # Specify your custom StorageClass here. Using the one which was cxreated as part of addon installation
    size: 10Gi  # Define the storage size you want
  # Create alert rules
serverFiles:
  alerting_rules.yml:
    groups:
      - name: Instances   # Name of the alerting group
        rules:
          - alert: InstanceDown # Alert name
            expr: up == 1  # Condition: An instance is down (not responding)
            for: 10s  # How long the condition must be met before firing
            labels:
              severity: critical # Custom label for categorization
            annotations:
              description: '{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 5 minutes.'
              summary: 'Instance {{ $labels.instance }} down'
          - alert: PodNotRunning
            expr: kube_pod_status_phase{phase!="Running"} > 0
            for: 1m
            labels:
              severity: critical
            annotations:
              summary: "Pod is not running"
              description: "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} is not running for more than 1 minutes."
          - alert: PodRestartedMoreThanTwice
            expr: kube_pod_container_status_restarts_total >= 2
            for: 1m  # Ensures the condition is true for 5 minutes before firing the alert
            labels:
                severity: critical
            annotations:
                summary: "Pod {{ $labels.pod }} has restarted more than 2 times"
                description: "The pod {{ $labels.pod }} in namespace {{ $labels.namespace }} has restarted more than 2 times in the last 5 minutes."
          - alert: NoAvailableReplicas
            expr: kube_deployment_status_replicas_available == 0
            for: 1m
            labels:
              severity: critical
            annotations:
              summary: "No available replicas for deployment"
              description: "Deployment {{ $labels.deployment }} in namespace {{ $labels.namespace }} has no available replicas."
          - alert: HighCpuUsage
            expr: (rate(container_cpu_usage_seconds_total[5m]) / kube_pod_container_resource_limits_cpu_cores) > 0.8
            for: 5m
            labels:
              severity: critical
            annotations:
              summary: "High CPU usage detected"
              description: "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} is using more than 80% of its allocated CPU."
alertmanager:
  extraArgs:
    cluster.advertise-address: "$(POD_IP):9094"
  persistence:
    enabled: true
    storageClass: "gp2"  # Specify your custom StorageClass here. Using the one which was cxreated as part of addon installation
    size: 10Gi  # Define the storage size you want

ingress.yaml

apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    meta.helm.sh/release-name: monitoring
    meta.helm.sh/release-namespace: monitoring
    nginx.ingress.kubernetes.io/rewrite-target: /$2
  labels:
    app.kubernetes.io/managed-by: Helm
  name: k8s-prometheus
  namespace: monitoring
spec:
  ingressClassName: nginx
  rules:
    - http:
        paths:
          - path: /prometheus(/|$)(.*)
            pathType: Prefix
            backend:
              service:
                name: prometheus-server   # Change this to your Prometheus service name
                port:
                  number: 80
          - path: /grafana(/|$)(.*)
            pathType: Prefix
            backend:
              service:
                name: grafana  # Change this to your Grafana service name
                port:
                  number: 80
          - path: /alertmanager
            pathType: Prefix
            backend:
              service:
                name: prometheus-alertmanager # Change this to your Alertmanager service name
                port:
                  number: 9093

--------------------------------------
kubectl logs prometheus-server-6d986b7978-94nml -n monitoring
Defaulted container "prometheus-server-configmap-reload" out of: prometheus-server-configmap-reload, prometheus-server
ts=2025-06-04T07:39:37.497492486Z level=info caller=/workspace/cmd/prometheus-config-reloader/main.go:148 msg="Starting prometheus-config-reloader" version="(version=0.83.0, branch=, revision=5cf2f5d)" build_context="(go=go1.24.3, platform=linux/amd64, user=, date=20250530-07:46:40, tags=unknown)"
ts=2025-06-04T07:39:37.50727007Z level=info caller=/workspace/internal/goruntime/cpu.go:27 msg="Leaving GOMAXPROCS=1: CPU quota undefined"
level=info ts=2025-06-04T07:39:37.511710755Z caller=reloader.go:282 msg="reloading via HTTP"
level=info ts=2025-06-04T07:39:37.511783545Z caller=reloader.go:330 msg="started watching config file and directories for changes" cfg= cfgDirs= out= dirs=/etc/config
ts=2025-06-04T07:39:37.511807709Z level=info caller=/workspace/cmd/prometheus-config-reloader/main.go:202 msg="Starting web server for metrics" listen=0.0.0.0:8080
ts=2025-06-04T07:39:37.512102674Z level=info caller=/go/pkg/mod/github.com/prometheus/exporter-toolkit@v0.14.0/web/tls_config.go:347 msg="Listening on" address=[::]:8080
ts=2025-06-04T07:39:37.512128065Z level=info caller=/go/pkg/mod/github.com/prometheus/exporter-toolkit@v0.14.0/web/tls_config.go:350 msg="TLS is disabled." http2=false address=[::]:8080
level=info ts=2025-06-04T07:42:37.519954286Z caller=reloader.go:548 msg="Reload triggered" cfg_in= cfg_out= cfg_dirs= watched_dirs=/etc/config


 kubectl logs -n ingress-nginx -l app.kubernetes.io/name=ingress-nginx
I0612 07:32:11.703282       6 controller.go:190] "Configuration changes detected, backend reload required"
I0612 07:32:11.764899       6 controller.go:210] "Backend successfully reloaded"
I0612 07:32:11.766257       6 event.go:364] Event(v1.ObjectReference{Kind:"Pod", Namespace:"ingress-nginx", Name:"ingress-nginx-controller-9f67d98bd-2qcx9", UID:"9e566589-503b-4609-ba65-c3467a24e615", APIVersion:"v1", ResourceVersion:"4138", FieldPath:""}): type: 'Normal' reason: 'RELOAD' NGINX reload triggered due to a change in configuration
I0612 07:32:47.961047       6 store.go:440] "Found valid IngressClass" ingress="monitoring/monitoring-ingress" ingressclass="nginx"
I0612 07:32:47.962115       6 event.go:364] Event(v1.ObjectReference{Kind:"Ingress", Namespace:"monitoring", Name:"monitoring-ingress", UID:"7d3fbeb7-caac-4733-a1e8-fc63a9809516", APIVersion:"networking.k8s.io/v1", ResourceVersion:"10525206", FieldPath:""}): type: 'Normal' reason: 'Sync' Scheduled for sync
I0612 07:32:47.962736       6 controller.go:190] "Configuration changes detected, backend reload required"
I0612 07:32:48.039533       6 controller.go:210] "Backend successfully reloaded"
I0612 07:32:48.042001       6 event.go:364] Event(v1.ObjectReference{Kind:"Pod", Namespace:"ingress-nginx", Name:"ingress-nginx-controller-9f67d98bd-2qcx9", UID:"9e566589-503b-4609-ba65-c3467a24e615", APIVersion:"v1", ResourceVersion:"4138", FieldPath:""}): type: 'Normal' reason: 'RELOAD' NGINX reload triggered due to a change in configuration
I0612 07:32:51.546682       6 status.go:304] "updating Ingress status" namespace="monitoring" ingress="monitoring-ingress" currentValue=null newValue=[{"hostname":"ac179b04319b94f71921bf2150a897f6-1409830007.us-east-1.elb.amazonaws.com"}]
I0612 07:32:51.555930       6 event.go:364] Event(v1.ObjectReference{Kind:"Ingress", Namespace:"monitoring", Name:"monitoring-ingress", UID:"7d3fbeb7-caac-4733-a1e8-fc63a9809516", APIVersion:"networking.k8s.io/v1", ResourceVersion:"10525230", FieldPath:""}): type: 'Normal' reason: 'Sync' Scheduled for sync
2025/06/12 07:34:04 [error] 1487#1487: *16684722 upstream timed out (110: Operation timed out) while connecting to upstream, client: 10.244.42.189, server: _, request: "GET /prometheus HTTP/1.1", upstream: "http://10.244.41.209:9090/", host: "ac179b04319b94f71921bf2150a897f6-1409830007.us-east-1.elb.amazonaws.com"
10.244.42.189 - - [12/Jun/2025:07:34:04 +0000] "GET /prometheus HTTP/1.1" 504 562 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/137.0.0.0 Safari/537.36 Edg/137.0.0.0" 569 15.001 [monitoring-prometheus-server-80] [] 10.244.41.209:9090, 10.244.41.209:9090, 10.244.41.209:9090 0, 0, 0 5.000, 5.001, 5.001 504, 504, 504 d500141a1c93d63898aa6f7c3b799101
2025/06/12 07:34:17 [error] 1487#1487: *16684722 upstream timed out (110: Operation timed out) while connecting to upstream, client: 10.244.42.189, server: _, request: "GET /prometheus HTTP/1.1", upstream: "http://10.244.41.209:9090/", host: "ac179b04319b94f71921bf2150a897f6-1409830007.us-east-1.elb.amazonaws.com"
2025/06/12 07:34:22 [error] 1487#1487: *16684722 upstream timed out (110: Operation timed out) while connecting to upstream, client: 10.244.42.189, server: _, request: "GET /prometheus HTTP/1.1", upstream: "http://10.244.41.209:9090/", host: "ac179b04319b94f71921bf2150a897f6-1409830007.us-east-1.elb.amazonaws.com"
2025/06/12 07:34:27 [error] 1487#1487: *16684722 upstream timed out (110: Operation timed out) while connecting to upstream, client: 10.244.42.189, server: _, request: "GET /prometheus HTTP/1.1", upstream: "http://10.244.41.209:9090/", host: "ac179b04319b94f71921bf2150a897f6-1409830007.us-east-1.elb.amazonaws.com"
10.244.42.189 - - [12/Jun/2025:07:34:27 +0000] "GET /prometheus HTTP/1.1" 504 562 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/137.0.0.0 Safari/537.36 Edg/137.0.0.0" 569 15.001 [monitoring-prometheus-server-80] [] 10.244.41.209:9090, 10.244.41.209:9090, 10.244.41.209:9090 0, 0, 0 5.000, 5.001, 5.000 504, 504, 504 fb168f9d03195b2e310f1e9f1bf36af0
2025/06/12 07:34:55 [error] 1487#1487: *16684722 upstream timed out (110: Operation timed out) while connecting to upstream, client: 10.244.42.189, server: _, request: "GET /prometheus HTTP/1.1", upstream: "http://10.244.41.209:9090/", host: "ac179b04319b94f71921bf2150a897f6-1409830007.us-east-1.elb.amazonaws.com"
2025/06/12 07:35:00 [error] 1487#1487: *16684722 upstream timed out (110: Operation timed out) while connecting to upstream, client: 10.244.42.189, server: _, request: "GET /prometheus HTTP/1.1", upstream: "http://10.244.41.209:9090/", host: "ac179b04319b94f71921bf2150a897f6-1409830007.us-east-1.elb.amazonaws.com"
2025/06/12 07:35:05 [error] 1487#1487: *16684722 upstream timed out (110: Operation timed out) while connecting to upstream, client: 10.244.42.189, server: _, request: "GET /prometheus HTTP/1.1", upstream: "http://10.244.41.209:9090/", host: "ac179b04319b94f71921bf2150a897f6-1409830007.us-east-1.elb.amazonaws.com"
10.244.42.189 - - [12/Jun/2025:07:35:05 +0000] "GET /prometheus HTTP/1.1" 504 562 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/137.0.0.0 Safari/537.36 Edg/137.0.0.0" 543 15.000 [monitoring-prometheus-server-80] [] 10.244.41.209:9090, 10.244.41.209:9090, 10.244.41.209:9090 0, 0, 0 5.000, 5.001, 5.000 504, 504, 504 0e0f257546a136d3311325da6fbfff0c

-------------
